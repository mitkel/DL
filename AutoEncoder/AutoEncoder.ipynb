{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "Refs: [DL book](https://www.deeplearningbook.org/contents/autoencoders.html); [inspired by](https://gist.github.com/AFAgarap/4f8a8d8edf352271fa06d85ba0361f26).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch       \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import special_ortho_group\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "class shallow_AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_width\"], out_features=kwargs[\"hidden_width\"], bias=kwargs[\"bias\"]\n",
    "        )\n",
    "        self.decoder_layer = nn.Linear(\n",
    "            in_features=kwargs[\"hidden_width\"], out_features=kwargs[\"input_width\"], bias=kwargs[\"bias\"]\n",
    "        )\n",
    "        \n",
    "        if 'linear' in kwargs:\n",
    "            self.linear = kwargs['linear']\n",
    "        else:\n",
    "            self.linear = False\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_layer(features)\n",
    "        if not self.linear:\n",
    "            activation = F.relu(activation)\n",
    "        reconstructed = self.decoder_layer(activation)\n",
    "        return reconstructed\n",
    "    \n",
    "def train(model, epochs, train_loader, optimizer, criterion, verbose=True):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for batch_features in train_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(batch_features)\n",
    "            train_loss = criterion(outputs, batch_features)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss += train_loss.item()\n",
    "\n",
    "        loss = loss / len(train_loader)\n",
    "        if verbose:\n",
    "            print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    if verbose:\n",
    "        print(\"===================\\n\")\n",
    "    return model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(**kwargs):\n",
    "    data0 = torch.randn(kwargs['samples'], kwargs['input_width'])\n",
    "    \n",
    "    if ('true_dim' in kwargs) and (kwargs['true_dim']<kwargs['input_width']):\n",
    "        data0[:,kwargs['true_dim']:] = 0\n",
    "        data0 = data0 @ special_ortho_group.rvs(kwargs['input_width'])\n",
    "        data0 = data0.float()\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        data0, batch_size=kwargs['batch_size']\n",
    "    )\n",
    "    \n",
    "    return data0, train_loader\n",
    "\n",
    "def PCA_compare(model, model_loss, data0, **kwargs):\n",
    "    U, s, V = np.linalg.svd(data0, full_matrices=False)\n",
    "\n",
    "    b=np.append(s[:kwargs['hidden_width']], np.zeros(len(s)-kwargs['hidden_width']))\n",
    "    L_opt = np.sum(s**2-b**2)*kwargs['batch_size']/kwargs['samples']\n",
    "\n",
    "    print(\"Model loss = \", model_loss)\n",
    "    print(\"PCA loss = \", L_opt)\n",
    "\n",
    "#     print(\"\\nModel params:\")\n",
    "#     for n, p in model.named_parameters():\n",
    "#         print(n, p)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"model estimate:\\n\", model(data0.to(device)).detach().cpu().numpy())\n",
    "        print(\"\\nPCA estimate:\\n\", U  @ np.diag(b) @ V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear activation implies that AutoEncoder = PCA\n",
    "\n",
    "### Underparametrization: hidden_width < true_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.063, -0.075, -1.177,  ..., -0.836, -0.472, -0.921],\n",
      "        [-1.006,  0.459, -0.354,  ..., -0.596, -0.040,  0.089],\n",
      "        [-0.846,  0.366, -1.969,  ...,  0.410,  0.420, -1.039],\n",
      "        ...,\n",
      "        [-0.295,  0.608, -0.234,  ..., -0.728, -1.632,  0.567],\n",
      "        [-0.998,  0.617, -1.223,  ...,  1.108,  0.195, -0.486],\n",
      "        [-0.446,  0.098,  0.699,  ..., -0.715,  0.286,  0.696]])\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "params = {\n",
    "    # data    \n",
    "    \"input_width\": 10, \n",
    "    \"samples\": 10000,\n",
    "    \"true_dim\":5,\n",
    "    \n",
    "    # model\n",
    "    \"hidden_width\":3,\n",
    "    \"linear\":True,\n",
    "    \"bias\": False,\n",
    "    \n",
    "    # training\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 100\n",
    "}\n",
    "\n",
    "data0, train_loader = gen_data(**params)\n",
    "print(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss =  194.5485447692871\n",
      "PCA loss =  194.2754940210744\n",
      "model estimate:\n",
      " [[-0.401  0.006 -1.003 ... -0.194  0.411 -0.803]\n",
      " [-0.468  0.349 -0.613 ... -0.172 -0.322 -0.025]\n",
      " [-1.057  0.418 -1.838 ... -0.335  0.163 -1.005]\n",
      " ...\n",
      " [-0.566  0.651 -0.149 ... -0.01  -0.897  0.643]\n",
      " [-1.227  0.674 -1.076 ...  0.195 -0.151 -0.451]\n",
      " [ 0.449 -0.082  0.283 ... -0.327 -0.384  0.499]]\n",
      "\n",
      "PCA estimate:\n",
      " [[-0.426  0.012 -1.006 ... -0.22   0.411 -0.819]\n",
      " [-0.482  0.354 -0.619 ... -0.191 -0.324 -0.027]\n",
      " [-1.063  0.418 -1.81  ... -0.397  0.141 -0.988]\n",
      " ...\n",
      " [-0.588  0.656 -0.16  ...  0.004 -0.878  0.627]\n",
      " [-1.219  0.671 -1.05  ...  0.151 -0.173 -0.434]\n",
      " [ 0.446 -0.079  0.27  ... -0.314 -0.377  0.501]]\n"
     ]
    }
   ],
   "source": [
    "model = shallow_AE(**params).to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=params['lr'])\n",
    "crit = nn.MSELoss(reduction='sum')\n",
    "\n",
    "model_trained, model_loss = train(model, params[\"epochs\"], train_loader, opt, crit, verbose=False)\n",
    "\n",
    "PCA_compare(model_trained, model_loss, data0, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is ReLU better than linear network = PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/100, loss = 495.942300\n",
      "epoch : 2/100, loss = 428.648785\n",
      "epoch : 3/100, loss = 376.046432\n",
      "epoch : 4/100, loss = 343.476146\n",
      "epoch : 5/100, loss = 329.040612\n",
      "epoch : 6/100, loss = 323.652824\n",
      "epoch : 7/100, loss = 321.606869\n",
      "epoch : 8/100, loss = 320.774074\n",
      "epoch : 9/100, loss = 320.367750\n",
      "epoch : 10/100, loss = 320.165468\n",
      "epoch : 11/100, loss = 320.046449\n",
      "epoch : 12/100, loss = 319.963443\n",
      "epoch : 13/100, loss = 319.896577\n",
      "epoch : 14/100, loss = 319.837841\n",
      "epoch : 15/100, loss = 319.786994\n",
      "epoch : 16/100, loss = 319.736912\n",
      "epoch : 17/100, loss = 319.688715\n",
      "epoch : 18/100, loss = 319.640509\n",
      "epoch : 19/100, loss = 319.595795\n",
      "epoch : 20/100, loss = 319.552417\n",
      "epoch : 21/100, loss = 319.513413\n",
      "epoch : 22/100, loss = 319.479889\n",
      "epoch : 23/100, loss = 319.448478\n",
      "epoch : 24/100, loss = 319.420873\n",
      "epoch : 25/100, loss = 319.393067\n",
      "epoch : 26/100, loss = 319.361754\n",
      "epoch : 27/100, loss = 319.331929\n",
      "epoch : 28/100, loss = 319.300603\n",
      "epoch : 29/100, loss = 319.268758\n",
      "epoch : 30/100, loss = 319.240201\n",
      "epoch : 31/100, loss = 319.214331\n",
      "epoch : 32/100, loss = 319.190931\n",
      "epoch : 33/100, loss = 319.169233\n",
      "epoch : 34/100, loss = 319.148475\n",
      "epoch : 35/100, loss = 319.130221\n",
      "epoch : 36/100, loss = 319.114512\n",
      "epoch : 37/100, loss = 319.098611\n",
      "epoch : 38/100, loss = 319.081689\n",
      "epoch : 39/100, loss = 319.063779\n",
      "epoch : 40/100, loss = 319.044728\n",
      "epoch : 41/100, loss = 319.023535\n",
      "epoch : 42/100, loss = 319.005337\n",
      "epoch : 43/100, loss = 318.984747\n",
      "epoch : 44/100, loss = 318.965988\n",
      "epoch : 45/100, loss = 318.944444\n",
      "epoch : 46/100, loss = 318.923985\n",
      "epoch : 47/100, loss = 318.902574\n",
      "epoch : 48/100, loss = 318.885622\n",
      "epoch : 49/100, loss = 318.868701\n",
      "epoch : 50/100, loss = 318.854570\n",
      "epoch : 51/100, loss = 318.842209\n",
      "epoch : 52/100, loss = 318.830236\n",
      "epoch : 53/100, loss = 318.820074\n",
      "epoch : 54/100, loss = 318.806927\n",
      "epoch : 55/100, loss = 318.794267\n",
      "epoch : 56/100, loss = 318.780625\n",
      "epoch : 57/100, loss = 318.769129\n",
      "epoch : 58/100, loss = 318.757114\n",
      "epoch : 59/100, loss = 318.746394\n",
      "epoch : 60/100, loss = 318.736334\n",
      "epoch : 61/100, loss = 318.724637\n",
      "epoch : 62/100, loss = 318.712843\n",
      "epoch : 63/100, loss = 318.702347\n",
      "epoch : 64/100, loss = 318.688561\n",
      "epoch : 65/100, loss = 318.677420\n",
      "epoch : 66/100, loss = 318.663342\n",
      "epoch : 67/100, loss = 318.647696\n",
      "epoch : 68/100, loss = 318.635291\n",
      "epoch : 69/100, loss = 318.620198\n",
      "epoch : 70/100, loss = 318.606357\n",
      "epoch : 71/100, loss = 318.595298\n",
      "epoch : 72/100, loss = 318.583858\n",
      "epoch : 73/100, loss = 318.572588\n",
      "epoch : 74/100, loss = 318.561998\n",
      "epoch : 75/100, loss = 318.551996\n",
      "epoch : 76/100, loss = 318.543551\n",
      "epoch : 77/100, loss = 318.534721\n",
      "epoch : 78/100, loss = 318.526347\n",
      "epoch : 79/100, loss = 318.517783\n",
      "epoch : 80/100, loss = 318.510642\n",
      "epoch : 81/100, loss = 318.502592\n",
      "epoch : 82/100, loss = 318.495597\n",
      "epoch : 83/100, loss = 318.489700\n",
      "epoch : 84/100, loss = 318.483033\n",
      "epoch : 85/100, loss = 318.477094\n",
      "epoch : 86/100, loss = 318.469944\n",
      "epoch : 87/100, loss = 318.465199\n",
      "epoch : 88/100, loss = 318.459981\n",
      "epoch : 89/100, loss = 318.454657\n",
      "epoch : 90/100, loss = 318.449632\n",
      "epoch : 91/100, loss = 318.444730\n",
      "epoch : 92/100, loss = 318.439539\n",
      "epoch : 93/100, loss = 318.434935\n",
      "epoch : 94/100, loss = 318.429171\n",
      "epoch : 95/100, loss = 318.422040\n",
      "epoch : 96/100, loss = 318.416331\n",
      "epoch : 97/100, loss = 318.410500\n",
      "epoch : 98/100, loss = 318.404679\n",
      "epoch : 99/100, loss = 318.398106\n",
      "epoch : 100/100, loss = 318.392065\n",
      "===================\n",
      "\n",
      "Model loss =  318.3920648193359\n",
      "PCA loss =  194.2754940210744\n",
      "model estimate:\n",
      " [[-0.287  0.126 -0.092 ...  0.161  0.033 -0.147]\n",
      " [-0.333  0.241  0.182 ... -0.007 -0.205  0.268]\n",
      " [-0.208  0.092 -0.066 ...  0.117  0.024 -0.106]\n",
      " ...\n",
      " [-0.268  0.275  0.389 ... -0.17  -0.371  0.586]\n",
      " [-0.49   0.216 -0.156 ...  0.275  0.057 -0.25 ]\n",
      " [-0.037  0.01   0.921 ... -0.571 -0.036  0.824]]\n",
      "\n",
      "PCA estimate:\n",
      " [[-0.426  0.012 -1.006 ... -0.22   0.411 -0.819]\n",
      " [-0.482  0.354 -0.619 ... -0.191 -0.324 -0.027]\n",
      " [-1.063  0.418 -1.81  ... -0.397  0.141 -0.988]\n",
      " ...\n",
      " [-0.588  0.656 -0.16  ...  0.004 -0.878  0.627]\n",
      " [-1.219  0.671 -1.05  ...  0.151 -0.173 -0.434]\n",
      " [ 0.446 -0.079  0.27  ... -0.314 -0.377  0.501]]\n"
     ]
    }
   ],
   "source": [
    "params[\"linear\"] = False\n",
    "\n",
    "model = shallow_AE(**params).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "crit = nn.MSELoss(reduction='sum')\n",
    "\n",
    "model_trained, model_loss = train(model, params[\"epochs\"], train_loader, opt, crit, verbose=True)\n",
    "\n",
    "PCA_compare(model_trained, model_loss, data0, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overparametrization: hidden_width > true_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.086, -0.022,  0.008,  ...,  0.061,  0.096, -0.016],\n",
      "        [-0.564,  0.570,  0.309,  ...,  1.265,  0.166,  0.295],\n",
      "        [-0.062, -0.014, -0.031,  ..., -0.161, -0.125, -0.002],\n",
      "        ...,\n",
      "        [-0.023,  0.216,  0.176,  ...,  0.802,  0.367,  0.098],\n",
      "        [ 0.838, -0.224,  0.069,  ...,  0.552,  0.919, -0.162],\n",
      "        [ 0.456, -0.407, -0.204,  ..., -0.811, -0.033, -0.215]])\n"
     ]
    }
   ],
   "source": [
    "params['true_dim'] = params['hidden_width']-1\n",
    "params['linear'] = True\n",
    "\n",
    "data0, train_loader = gen_data(**params)\n",
    "print(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss =  2.149715013204795e-12\n",
      "PCA loss =  6.664752785130093e-06\n",
      "model estimate:\n",
      " [[ 0.086 -0.022  0.008 ...  0.061  0.096 -0.016]\n",
      " [-0.564  0.57   0.309 ...  1.265  0.166  0.295]\n",
      " [-0.062 -0.014 -0.031 ... -0.161 -0.125 -0.002]\n",
      " ...\n",
      " [-0.023  0.216  0.176 ...  0.802  0.367  0.098]\n",
      " [ 0.838 -0.224  0.069 ...  0.552  0.919 -0.162]\n",
      " [ 0.456 -0.407 -0.204 ... -0.811 -0.033 -0.215]]\n",
      "\n",
      "PCA estimate:\n",
      " [[ 0.086 -0.022  0.008 ...  0.061  0.096 -0.016]\n",
      " [-0.564  0.57   0.309 ...  1.265  0.166  0.295]\n",
      " [-0.062 -0.014 -0.031 ... -0.161 -0.125 -0.002]\n",
      " ...\n",
      " [-0.023  0.216  0.176 ...  0.802  0.367  0.098]\n",
      " [ 0.838 -0.224  0.069 ...  0.552  0.919 -0.162]\n",
      " [ 0.456 -0.407 -0.204 ... -0.811 -0.033 -0.215]]\n"
     ]
    }
   ],
   "source": [
    "model = shallow_AE(**params).to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=params['lr'])\n",
    "crit = nn.MSELoss(reduction='sum')\n",
    "\n",
    "model_trained, model_loss = train(model, params[\"epochs\"], train_loader, opt, crit, verbose=False)\n",
    "\n",
    "model_loss\n",
    "PCA_compare(model_trained, model_loss, data0, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/100, loss = 74.141880\n",
      "epoch : 2/100, loss = 32.560665\n",
      "epoch : 3/100, loss = 32.535238\n",
      "epoch : 4/100, loss = 32.538647\n",
      "epoch : 5/100, loss = 32.503089\n",
      "epoch : 6/100, loss = 32.470490\n",
      "epoch : 7/100, loss = 32.461829\n",
      "epoch : 8/100, loss = 32.394977\n",
      "epoch : 9/100, loss = 32.324835\n",
      "epoch : 10/100, loss = 32.283395\n",
      "epoch : 11/100, loss = 32.229084\n",
      "epoch : 12/100, loss = 32.185357\n",
      "epoch : 13/100, loss = 32.153639\n",
      "epoch : 14/100, loss = 32.119113\n",
      "epoch : 15/100, loss = 32.088415\n",
      "epoch : 16/100, loss = 32.054099\n",
      "epoch : 17/100, loss = 32.000260\n",
      "epoch : 18/100, loss = 31.951956\n",
      "epoch : 19/100, loss = 31.910305\n",
      "epoch : 20/100, loss = 31.866338\n",
      "epoch : 21/100, loss = 31.858989\n",
      "epoch : 22/100, loss = 31.866579\n",
      "epoch : 23/100, loss = 31.857804\n",
      "epoch : 24/100, loss = 31.840992\n",
      "epoch : 25/100, loss = 31.832151\n",
      "epoch : 26/100, loss = 31.830606\n",
      "epoch : 27/100, loss = 31.834229\n",
      "epoch : 28/100, loss = 31.834116\n",
      "epoch : 29/100, loss = 31.834182\n",
      "epoch : 30/100, loss = 31.834183\n",
      "epoch : 31/100, loss = 31.834180\n",
      "epoch : 32/100, loss = 31.834179\n",
      "epoch : 33/100, loss = 31.834179\n",
      "epoch : 34/100, loss = 31.834180\n",
      "epoch : 35/100, loss = 31.834183\n",
      "epoch : 36/100, loss = 31.834185\n",
      "epoch : 37/100, loss = 31.831368\n",
      "epoch : 38/100, loss = 31.831218\n",
      "epoch : 39/100, loss = 31.831212\n",
      "epoch : 40/100, loss = 31.831217\n",
      "epoch : 41/100, loss = 31.831223\n",
      "epoch : 42/100, loss = 31.831230\n",
      "epoch : 43/100, loss = 31.831237\n",
      "epoch : 44/100, loss = 31.831247\n",
      "epoch : 45/100, loss = 31.831256\n",
      "epoch : 46/100, loss = 31.831266\n",
      "epoch : 47/100, loss = 31.831278\n",
      "epoch : 48/100, loss = 31.831290\n",
      "epoch : 49/100, loss = 31.831302\n",
      "epoch : 50/100, loss = 31.831315\n",
      "epoch : 51/100, loss = 31.831329\n",
      "epoch : 52/100, loss = 31.831343\n",
      "epoch : 53/100, loss = 31.831358\n",
      "epoch : 54/100, loss = 31.831374\n",
      "epoch : 55/100, loss = 31.831208\n",
      "epoch : 56/100, loss = 31.831229\n",
      "epoch : 57/100, loss = 31.832880\n",
      "epoch : 58/100, loss = 31.824796\n",
      "epoch : 59/100, loss = 31.819360\n",
      "epoch : 60/100, loss = 31.824828\n",
      "epoch : 61/100, loss = 31.826171\n",
      "epoch : 62/100, loss = 31.827423\n",
      "epoch : 63/100, loss = 31.827743\n",
      "epoch : 64/100, loss = 31.827288\n",
      "epoch : 65/100, loss = 31.827339\n",
      "epoch : 66/100, loss = 31.827365\n",
      "epoch : 67/100, loss = 31.827388\n",
      "epoch : 68/100, loss = 31.827412\n",
      "epoch : 69/100, loss = 31.827548\n",
      "epoch : 70/100, loss = 31.827671\n",
      "epoch : 71/100, loss = 31.827719\n",
      "epoch : 72/100, loss = 31.822543\n",
      "epoch : 73/100, loss = 31.812839\n",
      "epoch : 74/100, loss = 31.805707\n",
      "epoch : 75/100, loss = 31.810843\n",
      "epoch : 76/100, loss = 31.810861\n",
      "epoch : 77/100, loss = 31.811336\n",
      "epoch : 78/100, loss = 31.811410\n",
      "epoch : 79/100, loss = 31.811429\n",
      "epoch : 80/100, loss = 31.811443\n",
      "epoch : 81/100, loss = 31.811055\n",
      "epoch : 82/100, loss = 31.811025\n",
      "epoch : 83/100, loss = 31.811024\n",
      "epoch : 84/100, loss = 31.811036\n",
      "epoch : 85/100, loss = 31.810863\n",
      "epoch : 86/100, loss = 31.810882\n",
      "epoch : 87/100, loss = 31.810895\n",
      "epoch : 88/100, loss = 31.810909\n",
      "epoch : 89/100, loss = 31.810922\n",
      "epoch : 90/100, loss = 31.810936\n",
      "epoch : 91/100, loss = 31.810949\n",
      "epoch : 92/100, loss = 31.810963\n",
      "epoch : 93/100, loss = 31.810978\n",
      "epoch : 94/100, loss = 31.810992\n",
      "epoch : 95/100, loss = 31.811007\n",
      "epoch : 96/100, loss = 31.811021\n",
      "epoch : 97/100, loss = 31.811036\n",
      "epoch : 98/100, loss = 31.811051\n",
      "epoch : 99/100, loss = 31.811067\n",
      "epoch : 100/100, loss = 31.811082\n",
      "===================\n",
      "\n",
      "Model loss =  31.811082305908204\n",
      "PCA loss =  6.664752785130093e-06\n",
      "model estimate:\n",
      " [[ 0.103 -0.04  -0.002 ...  0.02   0.09  -0.025]\n",
      " [-0.879  0.696  0.319 ...  1.221 -0.101  0.375]\n",
      " [-0.058 -0.023 -0.037 ... -0.187 -0.135 -0.006]\n",
      " ...\n",
      " [-0.287  0.276  0.145 ...  0.587  0.058  0.144]\n",
      " [ 0.998 -0.386 -0.019 ...  0.191  0.871 -0.246]\n",
      " [ 0.15  -0.208 -0.13  ... -0.555 -0.149 -0.104]]\n",
      "\n",
      "PCA estimate:\n",
      " [[ 0.086 -0.022  0.008 ...  0.061  0.096 -0.016]\n",
      " [-0.564  0.57   0.309 ...  1.265  0.166  0.295]\n",
      " [-0.062 -0.014 -0.031 ... -0.161 -0.125 -0.002]\n",
      " ...\n",
      " [-0.023  0.216  0.176 ...  0.802  0.367  0.098]\n",
      " [ 0.838 -0.224  0.069 ...  0.552  0.919 -0.162]\n",
      " [ 0.456 -0.407 -0.204 ... -0.811 -0.033 -0.215]]\n"
     ]
    }
   ],
   "source": [
    "params['linear'] = False\n",
    "\n",
    "model = shallow_AE(**params).to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=params['lr'])\n",
    "crit = nn.MSELoss(reduction='sum')\n",
    "\n",
    "model_trained, model_loss = train(model, params[\"epochs\"], train_loader, opt, crit, verbose=True)\n",
    "\n",
    "model_loss\n",
    "PCA_compare(model_trained, model_loss, data0, **params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
