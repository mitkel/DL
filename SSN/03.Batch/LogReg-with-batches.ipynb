{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = datasets.load_digits()\n",
    "samples0, samples1 = 1500, 297\n",
    "\n",
    "data0, data1 = source.data[: samples0], source.data[samples0 :]\n",
    "target0, target1 = source.target[: samples0], source.target[samples0 :]\n",
    "design0, design1 = np.insert(data0, 0, 1., 1), np.insert(data1, 0, 1., 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0        3.292 0.588        1.592 0.798\n",
      "    1        1.367 0.814        1.968 0.774\n",
      "    2        0.755 0.888        1.736 0.848\n",
      "    3        0.351 0.938        1.372 0.855\n",
      "    4        0.165 0.964        1.131 0.889\n",
      "    5        0.126 0.975        1.159 0.886\n",
      "    6        0.061 0.983        1.001 0.865\n",
      "    7        0.065 0.977        1.029 0.879\n",
      "    8        0.046 0.984        0.838 0.896\n",
      "    9        0.032 0.991        0.957 0.889\n",
      "   10        0.032 0.992        0.823 0.899\n",
      "   11        0.018 0.995        0.895 0.892\n",
      "   12        0.012 0.997        0.889 0.892\n",
      "   13        0.009 0.998        0.891 0.896\n",
      "   14        0.007 0.999        0.896 0.896\n",
      "   15        0.007 0.999        0.917 0.896\n",
      "   16        0.005 0.999        0.914 0.902\n",
      "   17        0.005 0.999        0.923 0.902\n",
      "   18        0.004 0.999        0.919 0.899\n",
      "   19        0.004 0.999        0.930 0.899\n",
      "   20        0.004 0.999        0.931 0.899\n",
      "   21        0.003 0.999        0.934 0.899\n",
      "   22        0.003 0.999        0.937 0.899\n",
      "   23        0.003 1.000        0.938 0.899\n",
      "   24        0.003 1.000        0.940 0.899\n",
      "   25        0.003 1.000        0.942 0.899\n",
      "   26        0.002 1.000        0.943 0.899\n",
      "   27        0.002 1.000        0.944 0.899\n",
      "   28        0.002 1.000        0.944 0.902\n",
      "   29        0.002 1.000        0.945 0.902\n",
      "   30        0.002 1.000        0.946 0.902\n",
      "   31        0.002 1.000        0.946 0.902\n",
      "   32        0.002 1.000        0.946 0.902\n",
      "   33        0.002 1.000        0.946 0.902\n",
      "   34        0.002 1.000        0.947 0.902\n",
      "   35        0.002 1.000        0.947 0.902\n",
      "   36        0.002 1.000        0.947 0.902\n",
      "   37        0.002 1.000        0.947 0.902\n",
      "   38        0.002 1.000        0.947 0.902\n",
      "   39        0.002 1.000        0.947 0.902\n",
      "   40        0.002 1.000        0.948 0.902\n",
      "   41        0.002 1.000        0.948 0.902\n",
      "   42        0.001 1.000        0.948 0.906\n",
      "   43        0.001 1.000        0.948 0.906\n",
      "   44        0.001 1.000        0.948 0.909\n",
      "   45        0.001 1.000        0.948 0.909\n",
      "   46        0.001 1.000        0.949 0.909\n",
      "   47        0.001 1.000        0.949 0.909\n",
      "   48        0.001 1.000        0.949 0.909\n",
      "   49        0.001 1.000        0.949 0.909\n",
      "   50        0.001 1.000        0.950 0.909\n",
      "   51        0.001 1.000        0.950 0.909\n",
      "   52        0.001 1.000        0.950 0.909\n",
      "   53        0.001 1.000        0.951 0.909\n",
      "   54        0.001 1.000        0.951 0.906\n",
      "   55        0.001 1.000        0.952 0.906\n",
      "   56        0.001 1.000        0.952 0.906\n",
      "   57        0.001 1.000        0.953 0.906\n",
      "   58        0.001 1.000        0.953 0.906\n",
      "   59        0.001 1.000        0.954 0.906\n",
      "   60        0.001 1.000        0.954 0.906\n",
      "   61        0.001 1.000        0.955 0.906\n",
      "   62        0.001 1.000        0.955 0.906\n",
      "   63        0.001 1.000        0.956 0.906\n",
      "   64        0.001 1.000        0.956 0.906\n",
      "   65        0.001 1.000        0.957 0.906\n",
      "   66        0.001 1.000        0.958 0.906\n",
      "   67        0.001 1.000        0.958 0.906\n",
      "   68        0.001 1.000        0.959 0.906\n",
      "   69        0.001 1.000        0.959 0.906\n",
      "   70        0.001 1.000        0.960 0.906\n",
      "   71        0.001 1.000        0.961 0.906\n",
      "   72        0.001 1.000        0.961 0.906\n",
      "   73        0.001 1.000        0.962 0.906\n",
      "   74        0.001 1.000        0.963 0.906\n",
      "   75        0.001 1.000        0.963 0.906\n",
      "   76        0.001 1.000        0.964 0.906\n",
      "   77        0.001 1.000        0.965 0.906\n",
      "   78        0.001 1.000        0.965 0.906\n",
      "   79        0.001 1.000        0.966 0.906\n",
      "   80        0.001 1.000        0.967 0.906\n",
      "   81        0.001 1.000        0.968 0.906\n",
      "   82        0.001 1.000        0.968 0.906\n",
      "   83        0.001 1.000        0.969 0.906\n",
      "   84        0.001 1.000        0.970 0.906\n",
      "   85        0.001 1.000        0.970 0.906\n",
      "   86        0.001 1.000        0.971 0.906\n",
      "   87        0.001 1.000        0.972 0.906\n",
      "   88        0.001 1.000        0.972 0.906\n",
      "   89        0.001 1.000        0.973 0.906\n",
      "   90        0.001 1.000        0.974 0.906\n",
      "   91        0.001 1.000        0.975 0.906\n",
      "   92        0.001 1.000        0.975 0.906\n",
      "   93        0.001 1.000        0.976 0.906\n",
      "   94        0.001 1.000        0.977 0.906\n",
      "   95        0.001 1.000        0.977 0.906\n",
      "   96        0.001 1.000        0.978 0.906\n",
      "   97        0.001 1.000        0.979 0.906\n",
      "   98        0.001 1.000        0.979 0.906\n",
      "   99        0.001 1.000        0.980 0.906\n"
     ]
    }
   ],
   "source": [
    "features = 64\n",
    "classes = 10\n",
    "samples = 1797\n",
    "\n",
    "\n",
    "DESIGN0 = torch.tensor(design0, dtype = torch.float32)\n",
    "DESIGN1 = torch.tensor(design1, dtype = torch.float32)\n",
    "TARGET0 = torch.tensor(target0, dtype = torch.int64)\n",
    "TARGET1 = torch.tensor(target1, dtype = torch.int64)\n",
    "\n",
    "PARAM = torch.zeros(1+ features, classes, requires_grad = True)\n",
    "batch = 177\n",
    "opt = torch.optim.Adam([PARAM], lr = 0.055)\n",
    "\n",
    "for epoch in range(100):\n",
    "    LOSS0 = torch.zeros(())\n",
    "    ACC0 = torch.zeros(())\n",
    "    count0 = 0\n",
    "    \n",
    "    for index in range(0, samples0, batch):\n",
    "        opt.zero_grad()\n",
    "        DESIGN = DESIGN0[index : index + batch]\n",
    "        TARGET = TARGET0[index : index + batch]\n",
    "        count = TARGET.size(0)\n",
    "        \n",
    "        ACTIVATION = DESIGN @ PARAM\n",
    "        LOSS = torch.nn.functional.cross_entropy(ACTIVATION, TARGET)\n",
    "        LOSS0 += LOSS*count\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            VALUE = torch.argmax( ACTIVATION, 1)\n",
    "            ACC0 += torch.sum(VALUE == TARGET)\n",
    "            count0 += count\n",
    "        \n",
    "        LOSS.backward()\n",
    "        opt.step()\n",
    "    LOSS0 /= count0\n",
    "    ACC0 /= count0\n",
    "    \n",
    "    LOSS1 = torch.zeros(())\n",
    "    ACC1 = torch.zeros(())\n",
    "    count1 = 0\n",
    "    for index in range(0, samples1, batch):\n",
    "        with torch.no_grad():\n",
    "            DESIGN = DESIGN1[index : index + batch]\n",
    "            TARGET = TARGET1[index : index + batch]\n",
    "\n",
    "            ACTIVATION = DESIGN @ PARAM\n",
    "            LOSS1 += torch.nn.functional.cross_entropy(ACTIVATION, TARGET, reduction=\"sum\")\n",
    "        \n",
    "            VALUE = torch.argmax( ACTIVATION, 1)\n",
    "            ACC1 += torch.sum(VALUE == TARGET)\n",
    "            count1 += TARGET.size(0)\n",
    "    LOSS1 /= count1\n",
    "    ACC1 /= count1\n",
    "    \n",
    "    print(\"%5d %12.3f %4.3f %12.3f %4.3f\" % (epoch, LOSS0, ACC0, LOSS1, ACC1), flush = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
